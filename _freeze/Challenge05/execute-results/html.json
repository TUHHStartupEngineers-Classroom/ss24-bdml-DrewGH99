{
  "hash": "81dae6cb74583c95089b6c7cfdd5f449",
  "result": {
    "markdown": "---\ntitle: \"Performance Measures\"\nsubtitle: \"Business Decisions with Machine Learning\"\nauthor: \"Drew Gilmore\"\n---\n\n\n# Leaderboard Visualization\n## High max runtime to find deeplearning model\n\n::: {.cell hash='Challenge05_cache/html/unnamed-chunk-1_3e9651e68054d62693b3d506c1a2a6d1'}\n\n```{.r .cell-code}\nlibrary(tidyverse)\nlibrary(rsample)\nlibrary(recipes)\nlibrary(ggplot2)\nlibrary(h2o)\nlibrary(cowplot)\nlibrary(glue)\n\n# 1) Load the training & test dataset ----\n\nbackorders_tbl <- read_csv(\"Challenge 4/product_backorders.csv\")\n\n# 2) Specifiy the response and predictor variables ----\n\nprocessed_tbl <- backorders_tbl %>%\n  mutate_if(is.character, as.factor) \n\n# Split Data\nset.seed(seed = 1114)\nsplit_obj <- rsample::initial_split(processed_tbl, prop = 0.85)\n\n# Assign training and test data\ntrain_readable_tbl <- training(split_obj)\ntest_readable_tbl  <- testing(split_obj)\n\nrecipe_obj <- recipe(went_on_backorder ~ ., data = train_readable_tbl) %>%\n  step_zv(all_predictors()) %>%\n  prep()\n\ntrain_tbl <- bake(recipe_obj, new_data = train_readable_tbl)\ntest_tbl  <- bake(recipe_obj, new_data = test_readable_tbl)\n\n# 3) Run AutoML specifying the stopping criterion ----\n\n\nh2o.init()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n Connection successful!\n\nR is connected to the H2O cluster: \n    H2O cluster uptime:         7 hours 1 minutes \n    H2O cluster timezone:       Europe/Berlin \n    H2O data parsing timezone:  UTC \n    H2O cluster version:        3.44.0.3 \n    H2O cluster version age:    6 months and 5 days \n    H2O cluster name:           H2O_started_from_R_Drew_Gilmore_mvf229 \n    H2O cluster total nodes:    1 \n    H2O cluster total memory:   1.17 GB \n    H2O cluster total cores:    4 \n    H2O cluster allowed cores:  4 \n    H2O cluster healthy:        TRUE \n    H2O Connection ip:          localhost \n    H2O Connection port:        54321 \n    H2O Connection proxy:       NA \n    H2O Internal Security:      FALSE \n    R Version:                  R version 4.4.0 (2024-04-24 ucrt) \n```\n:::\n\n```{.r .cell-code}\nsplit_h2o <- h2o.splitFrame(as.h2o(train_tbl), ratios = c(0.85), seed = 1385)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\n  |                                                                            \n  |                                                                      |   0%\n  |                                                                            \n  |======================================================================| 100%\n```\n:::\n\n```{.r .cell-code}\ntrain_h2o <- split_h2o[[1]]\nvalid_h2o <- split_h2o[[2]]\ntest_h2o  <- as.h2o(test_tbl)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\n  |                                                                            \n  |                                                                      |   0%\n  |                                                                            \n  |======================================================================| 100%\n```\n:::\n\n```{.r .cell-code}\n# Set the target and predictors\ny <- \"went_on_backorder\"\nx <- setdiff(names(train_h2o), y)\n\nautoml_models_h2o <- h2o.automl(\n  x = x,\n  y = y,\n  training_frame    = train_h2o,\n  validation_frame  = valid_h2o,\n  leaderboard_frame = test_h2o,\n  max_runtime_secs  = 230 ,\n  nfolds            = 5 \n)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\n  |                                                                            \n  |                                                                      |   0%\n  |                                                                            \n  |==                                                                    |   2%\n23:38:12.928: User specified a validation frame with cross-validation still enabled. Please note that the models will still be validated using cross-validation only, the validation frame will be used to provide purely informative validation metrics on the trained models.\n23:38:12.931: AutoML: XGBoost is not available; skipping it.\n  |                                                                            \n  |=====                                                                 |   7%\n  |                                                                            \n  |======                                                                |   8%\n  |                                                                            \n  |=======                                                               |   9%\n  |                                                                            \n  |=======                                                               |  11%\n  |                                                                            \n  |=========                                                             |  12%\n  |                                                                            \n  |===========                                                           |  15%\n  |                                                                            \n  |===========                                                           |  16%\n  |                                                                            \n  |==============                                                        |  20%\n  |                                                                            \n  |===============                                                       |  21%\n  |                                                                            \n  |================                                                      |  22%\n  |                                                                            \n  |==================                                                    |  26%\n  |                                                                            \n  |===================                                                   |  27%\n  |                                                                            \n  |======================                                                |  31%\n  |                                                                            \n  |=======================                                               |  32%\n  |                                                                            \n  |========================                                              |  34%\n  |                                                                            \n  |=========================                                             |  35%\n  |                                                                            \n  |==========================                                            |  38%\n  |                                                                            \n  |=============================                                         |  41%\n  |                                                                            \n  |==============================                                        |  42%\n  |                                                                            \n  |================================                                      |  46%\n  |                                                                            \n  |=================================                                     |  47%\n  |                                                                            \n  |==================================                                    |  49%\n  |                                                                            \n  |====================================                                  |  51%\n  |                                                                            \n  |=====================================                                 |  53%\n  |                                                                            \n  |======================================                                |  54%\n  |                                                                            \n  |=======================================                               |  55%\n  |                                                                            \n  |==========================================                            |  59%\n  |                                                                            \n  |==========================================                            |  61%\n  |                                                                            \n  |============================================                          |  63%\n  |                                                                            \n  |=============================================                         |  64%\n  |                                                                            \n  |===============================================                       |  67%\n  |                                                                            \n  |================================================                      |  69%\n  |                                                                            \n  |===================================================                   |  73%\n  |                                                                            \n  |====================================================                  |  74%\n  |                                                                            \n  |=====================================================                 |  75%\n  |                                                                            \n  |=======================================================               |  78%\n  |                                                                            \n  |========================================================              |  80%\n  |                                                                            \n  |=========================================================             |  81%\n  |                                                                            \n  |=============================================================         |  88%\n  |                                                                            \n  |================================================================      |  91%\n  |                                                                            \n  |==================================================================    |  94%\n  |                                                                            \n  |====================================================================  |  97%\n  |                                                                            \n  |===================================================================== |  98%\n  |                                                                            \n  |======================================================================| 100%\n```\n:::\n\n```{.r .cell-code}\nautoml_models_h2o@leaderboard\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n                                                 model_id       auc   logloss\n1 StackedEnsemble_BestOfFamily_3_AutoML_8_20240625_233812 0.9563545 0.1747001\n2    StackedEnsemble_AllModels_2_AutoML_8_20240625_233812 0.9559671 0.1733442\n3    StackedEnsemble_AllModels_3_AutoML_8_20240625_233812 0.9544621 0.1774736\n4    StackedEnsemble_AllModels_1_AutoML_8_20240625_233812 0.9541242 0.1758023\n5 StackedEnsemble_BestOfFamily_2_AutoML_8_20240625_233812 0.9537802 0.1794509\n6                          GBM_4_AutoML_8_20240625_233812 0.9534626 0.1818408\n      aucpr mean_per_class_error      rmse        mse\n1 0.7619469            0.1286462 0.2310317 0.05337564\n2 0.7682825            0.1183912 0.2297318 0.05277670\n3 0.7620594            0.1250400 0.2312735 0.05348744\n4 0.7610027            0.1089044 0.2309589 0.05334203\n5 0.7486168            0.1450165 0.2336830 0.05460776\n6 0.7504468            0.1321882 0.2357761 0.05559038\n\n[25 rows x 7 columns] \n```\n:::\n:::\n\n\n# Tune a Model with grid search\n\n::: {.cell hash='Challenge05_cache/html/unnamed-chunk-2_d51665011bcc74ff7ad1a4ee3a0f1152'}\n\n```{.r .cell-code}\nlb <- automl_models_h2o@leaderboard\nlb_df <- as.data.frame(lb)\n\n# Identify the first deep learning model ----\ndl_model_id <- lb_df[grepl(\"DeepLearning\", lb_df$model_id), \"model_id\"][1]\n\nh2o.getModel(\"DeepLearning_1_AutoML_4_20240625_173758\") %>% \n  h2o.saveModel(path = \"h20_models/\", force = TRUE)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"C:\\\\Users\\\\Drew Gilmore\\\\Documents\\\\GitHub\\\\ss24-bdml-DrewGH99\\\\h20_models\\\\DeepLearning_1_AutoML_4_20240625_173758\"\n```\n:::\n\n```{.r .cell-code}\ndeeplearning_h2o <- h2o.loadModel(\"h20_models/DeepLearning_1_AutoML_4_20240625_173758\")\n\n# Grid Search to tune performance ----\n\ndeeplearning_grid_01 <- h2o.grid(\n  \n  algorithm = \"deeplearning\",\n  \n  grid_id = \"deeplearning_grid_01\",\n  \n  # predictor and response variables\n  x = x,\n  y = y,\n  \n  # training and validation frame and crossfold validation\n  training_frame   = train_h2o,\n  validation_frame = valid_h2o,\n  nfolds = 5,\n  \n  # Hyperparamters: Use deeplearning_h2o@allparameters to see all\n  hyper_params = list(\n    # Use some combinations (the first one was the original)\n    hidden = list(c(10, 10, 10), c(50, 20, 10), c(20, 20, 20)),\n    epochs = c(10, 50, 100)\n  )\n)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\n  |                                                                            \n  |                                                                      |   0%Error in .h2o.doSafeREST(h2oRestApiVersion = h2oRestApiVersion, urlSuffix = urlSuffix,  : \n  Unexpected CURL error: Timeout was reached: [localhost:54321] Connection timeout after 12833 ms\n[1] \"Job request failed Unexpected CURL error: Timeout was reached: [localhost:54321] Connection timeout after 12833 ms, will retry after 3s.\"\nError in .h2o.doSafeREST(h2oRestApiVersion = h2oRestApiVersion, urlSuffix = urlSuffix,  : \n  Unexpected CURL error: Timeout was reached: [localhost:54321] Connection timeout after 12879 ms\n[1] \"Job request failed Unexpected CURL error: Timeout was reached: [localhost:54321] Connection timeout after 12879 ms, will retry after 3s.\"\nError in .h2o.doSafeREST(h2oRestApiVersion = h2oRestApiVersion, urlSuffix = urlSuffix,  : \n  Unexpected CURL error: Timeout was reached: [localhost:54321] Connection timeout after 10098 ms\n[1] \"Job request failed Unexpected CURL error: Timeout was reached: [localhost:54321] Connection timeout after 10098 ms, will retry after 3s.\"\n\n  |                                                                            \n  |======================================================================| 100%\n```\n:::\n\n```{.r .cell-code}\n## Sort By AUC ----\n\nh2o.getGrid(grid_id = \"deeplearning_grid_01\", sort_by = \"auc\", decreasing = TRUE)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nH2O Grid Details\n================\n\nGrid ID: deeplearning_grid_01 \nUsed hyper parameters: \n  -  epochs \n  -  hidden \nNumber of models: 32 \nNumber of failed models: 0 \n\nHyper-Parameter Search Summary: ordered by decreasing auc\n     epochs       hidden                     model_ids     auc\n1 101.38334 [10, 10, 10]  deeplearning_grid_01_model_3 0.88021\n2 101.39586 [20, 20, 20] deeplearning_grid_01_model_23 0.87631\n3 101.38553 [50, 20, 10] deeplearning_grid_01_model_29 0.87334\n4 101.38846 [50, 20, 10] deeplearning_grid_01_model_20 0.86928\n5 101.40161 [20, 20, 20]  deeplearning_grid_01_model_9 0.86449\n\n---\n     epochs       hidden                     model_ids     auc\n27 10.40859 [50, 20, 10] deeplearning_grid_01_model_18 0.76553\n28 10.41403 [20, 20, 20]  deeplearning_grid_01_model_7 0.76447\n29 10.39812 [10, 10, 10] deeplearning_grid_01_model_15 0.76239\n30 10.39889 [10, 10, 10] deeplearning_grid_01_model_10 0.76001\n31 10.39951 [10, 10, 10]  deeplearning_grid_01_model_1 0.75827\n32 10.40757 [10, 10, 10] deeplearning_grid_01_model_24 0.70541\n```\n:::\n\n```{.r .cell-code}\n## Test model performance on test set ----\n\ndeeplearning_grid_01_model_1 <- h2o.getModel(\"deeplearning_grid_01_model_1\")\n\ndeeplearning_grid_01_model_1 %>% h2o.auc(train = T, valid = T, xval = T)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n    train     valid      xval \n0.7784159 0.7842656 0.7582657 \n```\n:::\n\n```{.r .cell-code}\n# Run it on the test data\ndeeplearning_grid_01_model_1 %>%\n  h2o.performance(newdata = as.h2o(test_tbl))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\n  |                                                                            \n  |                                                                      |   0%\n  |                                                                            \n  |======================================================================| 100%\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\nH2OBinomialMetrics: deeplearning\n\nMSE:  0.0966958\nRMSE:  0.3109595\nLogLoss:  0.3231114\nMean Per-Class Error:  0.2997247\nAUC:  0.7790075\nAUCPR:  0.3725879\nGini:  0.558015\n\nConfusion Matrix (vertical: actual; across: predicted) for F1-optimal threshold:\n         No Yes    Error       Rate\nNo     1866 627 0.251504  =627/2493\nYes     127 238 0.347945   =127/365\nTotals 1993 865 0.263821  =754/2858\n\nMaximum Metrics: Maximum metrics at their respective thresholds\n                        metric threshold       value idx\n1                       max f1  0.129831    0.386992 222\n2                       max f2  0.118904    0.544339 238\n3                 max f0point5  0.269064    0.419325 105\n4                 max accuracy  0.375472    0.880686  61\n5                max precision  0.974730    1.000000   0\n6                   max recall  0.008077    1.000000 388\n7              max specificity  0.974730    1.000000   0\n8             max absolute_mcc  0.241066    0.301144 125\n9   max min_per_class_accuracy  0.123017    0.706779 231\n10 max mean_per_class_accuracy  0.118904    0.718068 238\n11                     max tns  0.974730 2493.000000   0\n12                     max fns  0.974730  364.000000   0\n13                     max fps  0.000123 2493.000000 399\n14                     max tps  0.008077  365.000000 388\n15                     max tnr  0.974730    1.000000   0\n16                     max fnr  0.974730    0.997260   0\n17                     max fpr  0.000123    1.000000 399\n18                     max tpr  0.008077    1.000000 388\n\nGains/Lift Table: Extract with `h2o.gainsLift(<model>, <data>)` or `h2o.gainsLift(<model>, valid=<T/F>, xval=<T/F>)`\n```\n:::\n\n```{.r .cell-code}\n# Save stacked ensemble and GLM Model\n\nh2o.getModel(\"GLM_1_AutoML_4_20240625_173758\") %>% \n  h2o.saveModel(path = \"h20_models/\", force = TRUE)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"C:\\\\Users\\\\Drew Gilmore\\\\Documents\\\\GitHub\\\\ss24-bdml-DrewGH99\\\\h20_models\\\\GLM_1_AutoML_4_20240625_173758\"\n```\n:::\n\n```{.r .cell-code}\nh2o.getModel(\"StackedEnsemble_AllModels_2_AutoML_4_20240625_173758\") %>% \n  h2o.saveModel(path = \"h20_models/\", force = TRUE)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"C:\\\\Users\\\\Drew Gilmore\\\\Documents\\\\GitHub\\\\ss24-bdml-DrewGH99\\\\h20_models\\\\StackedEnsemble_AllModels_2_AutoML_4_20240625_173758\"\n```\n:::\n\n```{.r .cell-code}\n#H2O Performance\nstacked_ensemble_h2o <- h2o.loadModel(\"h20_models/StackedEnsemble_AllModels_2_AutoML_4_20240625_173758\")\ndeeplearning_h2o     <- h2o.loadModel(\"h20_models/DeepLearning_1_AutoML_4_20240625_173758\")\nglm_h2o              <- h2o.loadModel(\"h20_models/GLM_1_AutoML_4_20240625_173758\")\n\n## Create Performance Object ----\nperformance_h2o <- h2o.performance(stacked_ensemble_h2o, newdata = as.h2o(test_tbl))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\n  |                                                                            \n  |                                                                      |   0%\n  |                                                                            \n  |======================================================================| 100%\n```\n:::\n\n```{.r .cell-code}\n# Classifier Summary Metrics ----\n\nperformance_tbl <- performance_h2o %>%\n  h2o.metric() %>%\n  as.tibble() \n```\n:::\n\n\n# Visualize the trade off between the precision and the recall and the optimal threshold\n\n::: {.cell hash='Challenge05_cache/html/plot_2ff9af72f447a0e000243254c40da013'}\n\n```{.r .cell-code}\n# Visualize the trade off between the precision and the recall and the optimal threshold ----\ntheme_new <- theme(\n  legend.position  = \"bottom\",\n  legend.key       = element_blank(),,\n  panel.background = element_rect(fill   = \"transparent\"),\n  panel.border     = element_rect(color = \"black\", fill = NA, size = 0.5),\n  panel.grid.major = element_line(color = \"grey\", size = 0.333)\n) \n\n\nperformance_tbl %>%\n  ggplot(aes(x = threshold)) +\n  geom_line(aes(y = precision), color = \"blue\", size = 1) +\n  geom_line(aes(y = recall), color = \"red\", size = 1) +\n  \n  geom_vline(xintercept = h2o.find_threshold_by_max_metric(performance_h2o, \"f1\")) +\n  labs(title = \"Precision vs Recall\", y = \"value\") +\n  theme_new\n```\n\n::: {.cell-output-display}\n![](Challenge05_files/figure-html/plot-1.png){width=672}\n:::\n:::\n\n\n# ROC Plot\n\n::: {.cell hash='Challenge05_cache/html/plot2_ef85dd17b5a6a7ac5fba396e349cd3da'}\n\n```{.r .cell-code}\n# ROC Plot ----\n\npath <- \"h20_models/StackedEnsemble_AllModels_2_AutoML_4_20240625_173758\"\n\nload_model_performance_metrics <- function(path, test_tbl) {\n  \n  model_h2o <- h2o.loadModel(path)\n  perf_h2o  <- h2o.performance(model_h2o, newdata = as.h2o(test_tbl)) \n  \n  perf_h2o %>%\n    h2o.metric() %>%\n    as_tibble() %>%\n    mutate(auc = h2o.auc(perf_h2o)) %>%\n    select(tpr, fpr, auc)\n  \n}\n\nmodel_metrics_tbl <- fs::dir_info(path = \"h20_models/\") %>%\n  select(path) %>%\n  mutate(metrics = map(path, load_model_performance_metrics, test_tbl)) %>%\n  unnest(cols = metrics)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\n  |                                                                            \n  |                                                                      |   0%\n  |                                                                            \n  |======================================================================| 100%\n\n  |                                                                            \n  |                                                                      |   0%\n  |                                                                            \n  |======================================================================| 100%\n\n  |                                                                            \n  |                                                                      |   0%\n  |                                                                            \n  |======================================================================| 100%\n\n  |                                                                            \n  |                                                                      |   0%\n  |                                                                            \n  |======================================================================| 100%\n\n  |                                                                            \n  |                                                                      |   0%\n  |                                                                            \n  |======================================================================| 100%\n```\n:::\n\n```{.r .cell-code}\nmodel_metrics_tbl %>%\n  mutate(\n    # Extract the model names\n    path = str_split(path, pattern = \"/\", simplify = T)[,2] %>% as_factor(),\n    auc  = auc %>% round(3) %>% as.character() %>% as_factor()\n  ) %>%\n  ggplot(aes(fpr, tpr, color = path, linetype = auc)) +\n  geom_line(size = 1) +\n  \n  # just for demonstration purposes\n  geom_abline(color = \"red\", linetype = \"dotted\") +\n  \n  theme_new +\n  theme(\n    legend.direction = \"vertical\",\n  ) +\n  labs(\n    title = \"ROC Plot\",\n    subtitle = \"Performance of Some Top Performing Models\"\n  )\n```\n\n::: {.cell-output-display}\n![](Challenge05_files/figure-html/plot2-1.png){width=672}\n:::\n:::\n\n\n# Precision vs Recall\n\n::: {.cell hash='Challenge05_cache/html/plot3_c84e66a8c07f8431a709da9ac077f432'}\n\n```{.r .cell-code}\n# Precision vs Recall ----\n\nload_model_performance_metrics <- function(path, test_tbl) {\n  \n  model_h2o <- h2o.loadModel(path)\n  perf_h2o  <- h2o.performance(model_h2o, newdata = as.h2o(test_tbl)) \n  \n  perf_h2o %>%\n    h2o.metric() %>%\n    as_tibble() %>%\n    mutate(auc = h2o.auc(perf_h2o)) %>%\n    select(tpr, fpr, auc, precision, recall)\n  \n}\n\nmodel_metrics_tbl <- fs::dir_info(path = \"h20_models/\") %>%\n  select(path) %>%\n  mutate(metrics = map(path, load_model_performance_metrics, test_tbl)) %>%\n  unnest(cols = metrics)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\n  |                                                                            \n  |                                                                      |   0%\n  |                                                                            \n  |======================================================================| 100%\n\n  |                                                                            \n  |                                                                      |   0%\n  |                                                                            \n  |======================================================================| 100%\n\n  |                                                                            \n  |                                                                      |   0%\n  |                                                                            \n  |======================================================================| 100%\n\n  |                                                                            \n  |                                                                      |   0%\n  |                                                                            \n  |======================================================================| 100%\n\n  |                                                                            \n  |                                                                      |   0%\n  |                                                                            \n  |======================================================================| 100%\n```\n:::\n\n```{.r .cell-code}\nmodel_metrics_tbl %>%\n  mutate(\n    path = str_split(path, pattern = \"/\", simplify = T)[,2] %>% as_factor(),\n    auc  = auc %>% round(3) %>% as.character() %>% as_factor()\n  ) %>%\n  ggplot(aes(recall, precision, color = path, linetype = auc)) +\n  geom_line(size = 1) +\n  theme_new + \n  theme(\n    legend.direction = \"vertical\",\n  ) +\n  labs(\n    title = \"Precision vs Recall Plot\",\n    subtitle = \"Performance of Some Top Performing Models\"\n  )\n```\n\n::: {.cell-output-display}\n![](Challenge05_files/figure-html/plot3-1.png){width=672}\n:::\n:::\n\n\n# Gain Plot\n\n::: {.cell hash='Challenge05_cache/html/plot4_04e5b89582171e735665455320eb4a89'}\n\n```{.r .cell-code}\n# Gain & Lift ----\n\ngain_lift_tbl <- performance_h2o %>%\n  h2o.gainsLift() %>%\n  as.tibble()\n\n## Gain Chart ----\n\ngain_transformed_tbl <- gain_lift_tbl %>% \n  select(group, cumulative_data_fraction, cumulative_capture_rate, cumulative_lift) %>%\n  select(-contains(\"lift\")) %>%\n  mutate(baseline = cumulative_data_fraction) %>%\n  rename(gain     = cumulative_capture_rate) %>%\n  # prepare the data for the plotting (for the color and group aesthetics)\n  pivot_longer(cols = c(gain, baseline), values_to = \"value\", names_to = \"key\")\n\ngain_transformed_tbl %>%\n  ggplot(aes(x = cumulative_data_fraction, y = value, color = key)) +\n  geom_line(size = 1.5) +\n  labs(\n    title = \"Gain Chart\",\n    x = \"Cumulative Data Fraction\",\n    y = \"Gain\"\n  ) +\n  theme_new\n```\n\n::: {.cell-output-display}\n![](Challenge05_files/figure-html/plot4-1.png){width=672}\n:::\n:::\n\n\n# Lift Plot\n\n::: {.cell hash='Challenge05_cache/html/plot5_409190d169c2314aa689c4068587d07f'}\n\n```{.r .cell-code}\n## Lift Plot ----\n\nlift_transformed_tbl <- gain_lift_tbl %>% \n  select(group, cumulative_data_fraction, cumulative_capture_rate, cumulative_lift) %>%\n  select(-contains(\"capture\")) %>%\n  mutate(baseline = 1) %>%\n  rename(lift = cumulative_lift) %>%\n  pivot_longer(cols = c(lift, baseline), values_to = \"value\", names_to = \"key\")\n\nlift_transformed_tbl %>%\n  ggplot(aes(x = cumulative_data_fraction, y = value, color = key)) +\n  geom_line(size = 1.5) +\n  labs(\n    title = \"Lift Chart\",\n    x = \"Cumulative Data Fraction\",\n    y = \"Lift\"\n  ) +\n  theme_new\n```\n\n::: {.cell-output-display}\n![](Challenge05_files/figure-html/plot5-1.png){width=672}\n:::\n:::\n\n\n# Dashboard with Cowplot of Top 3 Leaders\n\n::: {.cell hash='Challenge05_cache/html/plot6_141e26acd509a4d041b970bc1fb89dae'}\n\n```{.r .cell-code}\n# Performance Visualization ----  \n\n#code re-used from lecture material to display cowplot of plots of top three model leaders\n\n# set values to test the function while building it\nh2o_leaderboard <- automl_models_h2o@leaderboard\nnewdata <- test_tbl\norder_by <- \"auc\"\nmax_models <- 4\nsize <- 1\n\nplot_h2o_performance <- function(h2o_leaderboard, newdata, order_by = c(\"auc\", \"logloss\"),\n                                 max_models = 3, size = 1.5) {\n  \n  # Inputs\n  \n  leaderboard_tbl <- h2o_leaderboard %>%\n    as_tibble() %>%\n    slice(1:max_models)\n  \n  newdata_tbl <- newdata %>%\n    as_tibble()\n  \n  # Selecting the first, if nothing is provided\n  order_by      <- tolower(order_by[[1]]) \n  \n  # Convert string stored in a variable to column name \n  order_by_expr <- rlang::sym(order_by)\n  \n  # Turn of the progress bars\n  h2o.no_progress()\n  \n  # Model metrics\n  \n  get_model_performance_metrics <- function(model_id, test_tbl) {\n    \n    model_h2o <- h2o.getModel(model_id)\n    perf_h2o  <- h2o.performance(model_h2o, newdata = as.h2o(test_tbl))\n    \n    perf_h2o %>%\n      h2o.metric() %>%\n      as.tibble() %>%\n      select(threshold, tpr, fpr, precision, recall)\n    \n  }\n  \n  model_metrics_tbl <- leaderboard_tbl %>%\n    mutate(metrics = map(model_id, get_model_performance_metrics, newdata_tbl)) %>%\n    unnest(cols = metrics) %>%\n    mutate(\n      model_id = as_factor(model_id) %>% \n        fct_reorder(!! order_by_expr, \n                    .desc = ifelse(order_by == \"auc\", TRUE, FALSE)),\n      auc      = auc %>% \n        round(3) %>% \n        as.character() %>% \n        as_factor() %>% \n        fct_reorder(as.numeric(model_id)),\n      logloss  = logloss %>% \n        round(4) %>% \n        as.character() %>% \n        as_factor() %>% \n        fct_reorder(as.numeric(model_id))\n    )\n  \n  \n  # ROC Plot\n  \n  p1 <- model_metrics_tbl %>%\n    ggplot(aes(fpr, tpr, color = model_id, linetype = !! order_by_expr)) +\n    geom_line(size = size) +\n    theme_new +\n    labs(title = \"ROC\", x = \"FPR\", y = \"TPR\") +\n    theme(legend.direction = \"vertical\") \n  \n  \n  # Precision vs Recall\n  \n  p2 <- model_metrics_tbl %>%\n    ggplot(aes(recall, precision, color = model_id, linetype = !! order_by_expr)) +\n    geom_line(size = size) +\n    theme_new +\n    labs(title = \"Precision Vs Recall\", x = \"Recall\", y = \"Precision\") +\n    theme(legend.position = \"none\") \n  \n  \n  # Gain / Lift\n  \n  get_gain_lift <- function(model_id, test_tbl) {\n    \n    model_h2o <- h2o.getModel(model_id)\n    perf_h2o  <- h2o.performance(model_h2o, newdata = as.h2o(test_tbl)) \n    \n    perf_h2o %>%\n      h2o.gainsLift() %>%\n      as.tibble() %>%\n      select(group, cumulative_data_fraction, cumulative_capture_rate, cumulative_lift)\n    \n  }\n  \n  gain_lift_tbl <- leaderboard_tbl %>%\n    mutate(metrics = map(model_id, get_gain_lift, newdata_tbl)) %>%\n    unnest(cols = metrics) %>%\n    mutate(\n      model_id = as_factor(model_id) %>% \n        fct_reorder(!! order_by_expr, \n                    .desc = ifelse(order_by == \"auc\", TRUE, FALSE)),\n      auc  = auc %>% \n        round(3) %>% \n        as.character() %>% \n        as_factor() %>% \n        fct_reorder(as.numeric(model_id)),\n      logloss = logloss %>% \n        round(4) %>% \n        as.character() %>% \n        as_factor() %>% \n        fct_reorder(as.numeric(model_id))\n    ) %>%\n    rename(\n      gain = cumulative_capture_rate,\n      lift = cumulative_lift\n    ) \n  \n  #  Gain Plot\n  \n  p3 <- gain_lift_tbl %>%\n    ggplot(aes(cumulative_data_fraction, gain, \n               color = model_id, linetype = !! order_by_expr)) +\n    geom_line(size = size,) +\n    geom_segment(x = 0, y = 0, xend = 1, yend = 1, \n                 color = \"red\", size = size, linetype = \"dotted\") +\n    theme_new +\n    expand_limits(x = c(0, 1), y = c(0, 1)) +\n    labs(title = \"Gain\",\n         x = \"Cumulative Data Fraction\", y = \"Gain\") +\n    theme(legend.position = \"none\")\n  \n  # Lift Plot\n  \n  p4 <- gain_lift_tbl %>%\n    ggplot(aes(cumulative_data_fraction, lift, \n               color = model_id, linetype = !! order_by_expr)) +\n    geom_line(size = size) +\n    geom_segment(x = 0, y = 1, xend = 1, yend = 1, \n                 color = \"red\", size = size, linetype = \"dotted\") +\n    theme_new +\n    expand_limits(x = c(0, 1), y = c(0, 1)) +\n    labs(title = \"Lift\",\n         x = \"Cumulative Data Fraction\", y = \"Lift\") +\n    theme(legend.position = \"none\") \n  \n  \n  # Combine using cowplot\n  \n  # cowplot::get_legend extracts a legend from a ggplot object\n  p_legend <- get_legend(p1)\n  # Remove legend from p1\n  p1 <- p1 + theme(legend.position = \"none\")\n  \n  # cowplot::plt_grid() combines multiple ggplots into a single cowplot object\n  p <- cowplot::plot_grid(p1, p2, p3, p4, ncol = 2)\n  \n  # cowplot::ggdraw() sets up a drawing layer\n  p_title <- ggdraw() + \n    \n    # cowplot::draw_label() draws text on a ggdraw layer / ggplot object\n    draw_label(\"H2O Model Metrics\", size = 18, fontface = \"bold\", \n               color = \"#2C3E50\")\n  \n  p_subtitle <- ggdraw() + \n    draw_label(glue(\"Ordered by {toupper(order_by)}\"), size = 10,  \n               color = \"#2C3E50\")\n  \n  # Combine everything\n  ret <- plot_grid(p_title, p_subtitle, p, p_legend, \n                   \n                   # Adjust the relative spacing, so that the legends always fits\n                   ncol = 1, rel_heights = c(0.05, 0.05, 1, 0.05 * max_models))\n  \n  h2o.show_progress()\n  \n  return(ret)\n  \n}\n\nautoml_models_h2o@leaderboard %>%\n  plot_h2o_performance(newdata = test_tbl, order_by = \"logloss\", \n                       size = 0.5, max_models = 4)\n```\n\n::: {.cell-output-display}\n![](Challenge05_files/figure-html/plot6-1.png){width=672}\n:::\n:::",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}